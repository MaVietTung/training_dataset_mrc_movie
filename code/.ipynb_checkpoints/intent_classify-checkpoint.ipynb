{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "# from custom_model import LSTM_fixed_len\n",
    "from custom_model import AttentionModel\n",
    "\n",
    "from custom_dataset import customDataset\n",
    "from sklearn.utils import class_weight\n",
    "from pyvi import ViTokenizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample,shuffle\n",
    "\n",
    "from utils import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "model_path = '../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(os.path.join(data_path,'question_livestream_label.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taindp/opt/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thầy cho em hỏi nếu mình đã trúng tuyển chương...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>cho em hỏi chương trình chất lượng cao ở bách ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cho em hỏi nếu em đã trúng tuyển chương trình ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>cho em hỏi chỉ tiêu ngành khoa học máy tính nă...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>cho em hỏi ngành khoa học máy tính có những hì...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2</td>\n",
       "      <td>cho em hỏi về ngành kỹ thuật hoá học và cơ hội...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2</td>\n",
       "      <td>cho em xin giới thiệu về ngành kỹ thuật robot ạ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2</td>\n",
       "      <td>ngành khoa học máy tính sau này ra làm công vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2</td>\n",
       "      <td>em muốn học tự động hoá thì tương lai sẽ có ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2</td>\n",
       "      <td>dạ cho em hỏi về đầu ra và cơ hội nghề nghiệp ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content\n",
       "0        0  thầy cho em hỏi nếu mình đã trúng tuyển chương...\n",
       "2        0  cho em hỏi chương trình chất lượng cao ở bách ...\n",
       "3        0  cho em hỏi nếu em đã trúng tuyển chương trình ...\n",
       "5        1  cho em hỏi chỉ tiêu ngành khoa học máy tính nă...\n",
       "8        1  cho em hỏi ngành khoa học máy tính có những hì...\n",
       "..     ...                                                ...\n",
       "428      2  cho em hỏi về ngành kỹ thuật hoá học và cơ hội...\n",
       "429      2    cho em xin giới thiệu về ngành kỹ thuật robot ạ\n",
       "430      2  ngành khoa học máy tính sau này ra làm công vi...\n",
       "431      2  em muốn học tự động hoá thì tương lai sẽ có ng...\n",
       "432      2  dạ cho em hỏi về đầu ra và cơ hội nghề nghiệp ...\n",
       "\n",
       "[346 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df_[df_['label']!=0]\n",
    "question['label'] = [item-1 for item in list(question['label'])]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def upsample(df,target_num):\n",
    "class_0 = question[question['label']==0]\n",
    "class_0_upsampled = resample(class_0,random_state=42,n_samples=90-len(class_0),replace=True)\n",
    "question_upsampled = pd.concat([question, class_0_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    148\n",
       "1    122\n",
       "0     90\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = question_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question['length'] = [len(item) for item in list(question['content'])]\n",
    "question['num_word'] = [len(item.split(' ')) for item in list(question['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.336111111111112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(question['num_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([36., 81., 77., 61., 39., 30., 25.,  5.,  3.,  3.]),\n",
       " array([ 6. ,  9.6, 13.2, 16.8, 20.4, 24. , 27.6, 31.2, 34.8, 38.4, 42. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(question['num_word']), bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    list_token = ViTokenizer.tokenize(text)\n",
    "    return list_token.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for index, row in question.iterrows():\n",
    "    counts.update(tokenize(row['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 535\n",
      "num_words after: 323\n"
     ]
    }
   ],
   "source": [
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=75):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "#     print(len(enc1))\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "#     print(len(encoded))\n",
    "    return [encoded]\n",
    "#     return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>length</th>\n",
       "      <th>num_word</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>thầy cho em hỏi nếu mình đã trúng tuyển chương...</td>\n",
       "      <td>159</td>\n",
       "      <td>33</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>cho em hỏi chương trình chất lượng cao ở bách ...</td>\n",
       "      <td>106</td>\n",
       "      <td>24</td>\n",
       "      <td>[[3, 4, 5, 10, 15, 16, 24, 25, 26, 21, 27, 28,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>cho em hỏi nếu em đã trúng tuyển chương trình ...</td>\n",
       "      <td>148</td>\n",
       "      <td>31</td>\n",
       "      <td>[[3, 4, 5, 6, 4, 8, 9, 10, 11, 12, 4, 13, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>cho em hỏi chỉ tiêu ngành khoa học máy tính nă...</td>\n",
       "      <td>108</td>\n",
       "      <td>25</td>\n",
       "      <td>[[3, 4, 5, 35, 36, 37, 38, 39, 40, 41, 42, 43,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>cho em hỏi ngành khoa học máy tính có những hì...</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>[[3, 4, 5, 36, 37, 38, 26, 46, 47, 48, 49, 23,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content  length  num_word  \\\n",
       "0      0  thầy cho em hỏi nếu mình đã trúng tuyển chương...     159        33   \n",
       "2      0  cho em hỏi chương trình chất lượng cao ở bách ...     106        24   \n",
       "3      0  cho em hỏi nếu em đã trúng tuyển chương trình ...     148        31   \n",
       "5      1  cho em hỏi chỉ tiêu ngành khoa học máy tính nă...     108        25   \n",
       "8      1  cho em hỏi ngành khoa học máy tính có những hì...      70        16   \n",
       "\n",
       "                                             encoded  \n",
       "0  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 1...  \n",
       "2  [[3, 4, 5, 10, 15, 16, 24, 25, 26, 21, 27, 28,...  \n",
       "3  [[3, 4, 5, 6, 4, 8, 9, 10, 11, 12, 4, 13, 34, ...  \n",
       "5  [[3, 4, 5, 35, 36, 37, 38, 39, 40, 41, 42, 43,...  \n",
       "8  [[3, 4, 5, 36, 37, 38, 26, 46, 47, 48, 49, 23,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 22\n",
    "question['encoded'] = question['content'].apply(lambda x: np.array(encode_sentence(x,vocab2index,N)))\n",
    "question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(question['encoded'])\n",
    "y = list(question['label'])\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=(1-0.693))\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 90)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taindp/opt/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass classes=[0, 1, 2], y=[2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 0, 2, 1, 1, 1, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 1, 2, 2, 2, 2, 1, 1, 0, 2, 2, 2, 1, 2, 1, 2, 0, 0, 1, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 1, 2, 0, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 1, 0, 0, 2, 2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 1, 1, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 0, 2, 0, 2, 1, 2, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 0, 2, 2, 2, 2, 1, 2, 2, 1, 1, 0, 1, 1, 2, 2, 1, 0, 2, 2, 1, 1, 1, 2, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.3636, 0.9091, 0.8571])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',np.unique(y).tolist(),y)\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train).tolist(),y_train)\n",
    "class_weights = torch.tensor(class_weights,dtype=torch.float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = customDataset(X_train, y_train)\n",
    "valid_ds = customDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 360*0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a,b in val_dl:\n",
    "#     print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel(\n",
    "                    batch_size=batch_size, \n",
    "                    output_size=class_weights.shape[0], \n",
    "                    hidden_size=128, \n",
    "                    vocab_size=vocab_size, \n",
    "                    embedding_length=256\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (word_embeddings): Embedding(325, 256)\n",
       "  (lstm): LSTM(256, 128)\n",
       "  (label): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.train()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,train_dl,optimizer,criterion,writer,epoch):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    list_pred = []\n",
    "    for x, y in train_dl:\n",
    "        y = y.type(torch.int64)\n",
    "        x = x.long()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        loss = criterion(y_pred,y)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        list_pred.append(y_pred.argmax())\n",
    "    return epoch_loss / len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, valid_dl,criterion,writer,epoch):\n",
    "    model.eval()\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    list_true = []\n",
    "    list_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_dl:\n",
    "            y = y.type(torch.int64)\n",
    "            x = x.long()\n",
    "            y_hat = model(x)\n",
    "#             acc = binary_accuracy(y_hat,y)\n",
    "            loss = criterion(y_hat,y)\n",
    "            writer.add_scalar(\"Loss/valid\", loss, epoch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_one_epoch() missing 2 required positional arguments: 'writer' and 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0a192141f0aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch-{0} lr: {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_one_epoch() missing 2 required positional arguments: 'writer' and 'epoch'"
     ]
    }
   ],
   "source": [
    "\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_loss=[]\n",
    "acc_max = 0\n",
    "\n",
    "valid_loss_min = 1.\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    train_loss = train_one_epoch(model,train_dl,optimizer,criterion,writer,epoch)\n",
    "    valid_loss = evaluate_one_epoch(model, val_dl,criterion,writer,epoch)\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Valid Loss: {valid_loss:.3f}')\n",
    "    \n",
    "    if valid_loss < valid_loss_min:\n",
    "        valis_loss_min = valid_loss\n",
    "        checkpoint = {'model': model,\n",
    "          'state_dict': model.state_dict(),\n",
    "          'optimizer' : optimizer.state_dict()}\n",
    "#         valis_loss_save = str(valis_loss_min).replace('.','_')[:10]\n",
    "        torch.save(checkpoint, os.path.join(model_path,'checkpoint_{}.pth'.format(valis_loss_min)))\n",
    "\n",
    "    loss.append(train_loss)\n",
    "#     acc.append(train_acc)\n",
    "    val_loss.append(valid_loss)\n",
    "#     exp_lr_scheduler.step()\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = {'model': model_fixed,\n",
    "#       'state_dict': model_fixed.state_dict(),\n",
    "#       'optimizer' : optimizer.state_dict()}\n",
    "\n",
    "# torch.save(checkpoint, os.path.join(model_path,'model_jun24.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = load_checkpoint(os.path.join(model_path,'checkpoint_0.40826831261316937.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred = []\n",
    "list_true = []\n",
    "for x,y in val_dl:\n",
    "    x = x.long()\n",
    "    print(x.shape)\n",
    "    pred = load_model(x)\n",
    "    for item in pred:\n",
    "#         print(item.argmax())\n",
    "        list_pred.append(item.argmax().item())\n",
    "    for true in y:\n",
    "        list_true.append(true.item())\n",
    "#         print(true.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(list_pred,list_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(f1_score(list_true, list_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = os.path.join(model_path,'model_intent.pth')\n",
    "# joblib.dump(load_model, filename)\n",
    "# # with open('vectorizer.pickle', 'wb') as handle:\n",
    "# #     pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# url = 'https://api-intent.herokuapp.com/predict'\n",
    "# pred = requests.post(url,json={'message':'ad cho em hỏi chương trình tiên tiến với chất lượng cao khác nhau thế nào ạ'})\n",
    "# print(pred.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in val_dl:\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab2index,os.path.join(model_path,'vocab_12jul.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = torch.load(os.path.join(model_path,'vocab_12jul.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_enc = torch.zeros([29,22])\n",
    "# pad_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sent = 'ad cho em hỏi ngành điện tử viễn thông có ổn không ạ'\n",
    "test_enc =  torch.from_numpy(encode_sentence(test_sent, vocab2index, N)[0].astype(np.float32))\n",
    "test_enc = torch.reshape(test_enc,(1,N))\n",
    "test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_pad = torch.cat([test_enc,pad_enc])\n",
    "test_enc_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "preds = load_model(test_enc_pad.long())\n",
    "prop_preds = nn.functional.softmax(preds,dim=1)\n",
    "print(prop_preds[0])\n",
    "pred_label = prop_preds.argmax().item()\n",
    "pred_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
